<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Jiayuan Fan-Fudan University</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="#research">Research</a></div>
<div class="menu-item"><a href="https://fansarah.github.io/publications">Publication</a></div>
<!-- <div class="menu-item"><a href="#team">Team</a></div> -->
<div class="menu-item"><a href="#teaching">Teaching</a></div>  
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Jiayuan Fan</h1>
</div>
<table class="imgtable"><tr><td>
<img src="avat.jpg" alt="" width="160px" height="180px" />&nbsp;</td>
<td align="left"><p>
<font size="4">
<b>Jiayuan Fan (范佳媛)</b><br />
Associate Professor<br />  
College of Intelligent Robotics and Advanced Manufacturing<br />
Fudan University<br />
</font></p>
<p>
  <br />
Email: jyfan@fudan.edu.cn<br />
[<a href="https://scholar.google.com.hk/citations?user=gsLd2ccAAAAJ&hl=zh-CN">Google Scholar</a>] [<a href="https://dblp.org/pid/76/10698-1.html">DBLP</a>] [<a href="https://faet.fudan.edu.cn/49/ba/c23898a674234/page.htm">Website</a>]</p>
</td></tr></table>
<h2>Biography</h2>
<p>I'm an Associate Professor at the College of Intelligent Robotics and Advanced Manufacturing, Fudan University. I'm also an IEEE Senior Member. Since joining Fudan University in 2019, I have been selected for Shanghai High-Level Talent Plan, and Pujiang Talent Program. Prior to this, I was a Scientist at A*STAR Institute for Infocomm Research (A*STAR I²R) from 2014 to 2019. I obtained my PhD in Electrical and Electronic Engineering from Nanyang Technological University (NTU) in 2015, under the supervision of Prof. Alex Kot. My current research interests lie in embodied vision language model, low-altitude visual computing, generative foundation models, particularly in the applications of robotics and UAV.</p>
<h2>News</h2>
<ul> 
<li><p>2025-05: We are glad to announce that our new paper for multi-task prediction <a href="https://ieeexplore.ieee.org/document/10870147">"BridgeNet: Comprehensive and Effective Feature Interactions via Bridge Feature for Multi-Task Dense Predictions"</a> is published in TPAMI'25.
</li>    
<li><p>2024-12: Our mamba-based work for HSI Classification <a href="https://ieeexplore.ieee.org/document/10798573">"DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for Hyperspectral Image Classification"</a> is published in TGRS'24. Code is available <a href="https://github.com/sjmFDU/DualMamba">here</a>.
</li>   
<li><p>2024-07: <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03701.pdf">"Motionchain: Conversational Motion Controllers via Multimodal Prompts"</a> is accepted by ECCV'24.
</li> 
<li><p>2024-06: It is pleased to announce that our language-guided distillation method for model pre-training <a href="https://ieeexplore.ieee.org/document/10551493">"Lightweight Model Pre-Training via Language Guided Knowledge Distillation"</a> is published in TMM'24.
</li> 
<li><p>2024-05: We have one paper on <a href="https://ieeexplore.ieee.org/document/10542168">"Exploring Multi-Timestep Multi-Stage Diffusion Features for Hyperspectral Image Classification"</a>, accepted by TGRS'24 and one paper on <a href="https://ieeexplore.ieee.org/document/10516600">"Few-Shot Cross-Domain Object Detection With Instance-Level Prototype-Based Meta-Learning"</a>, published in TCSVT'24.
</li>  
<li><p>2024-04: <a href="https://ieeexplore.ieee.org/document/10510343">"U²ConvFormer: Marrying and Evolving Nested U-Net and Scale-Aware Transformer for Hyperspectral Image Classification"</a> is published in TGRS'24.
</li>
<li><p>2024-02: We are pleased to announce that our work on 3D environment understanding <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.pdf">"Ll3da: Visual Interactive Instruction Tuning for Omni-3D Understanding Reasoning and Planning"</a> is accepted by CVPR'24.
</li>
<li><p>2024-01: Our paper on <a href="https://ieeexplore.ieee.org/document/10611709">"Through the Real World Haze Scenes: Navigating the Synthetic-to-Real Gap in Challenging Image Dehazing"</a> is accepted by ICRA'24.
</li> 
<li><p>2023-12: We have one paper on <a href="https://dl.acm.org/doi/10.1609/aaai.v38i7.28481">"Pm-inr: Prior-rich Multi-modal Implicit Large-scale Scene Neural Representation"</a>, accepted by AAAI'24 and one paper on <a href="https://ieeexplore.ieee.org/document/10342783">"CenterCoop: Center-Based Feature Aggregation for Communication-Efficient Vehicle-Infrastructure Cooperative 3D Object Detection"</a>, accepted by RAL'23.
</li> 
<li><p>2023-09: We have one paper on <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/0073cc73e1873b35345209b50a3dab66-Paper-Conference.pdf">"Point diffusion implicit function for large-scale scene neural representation"</a>, accepted by NeurIPS'23 and one paper on <a href="https://ieeexplore.ieee.org/document/10255628">"DeNKD: Decoupled Non-Target Knowledge Distillation for Complementing Transformer-Based Unsupervised Domain Adaptation"</a>, accepted by TCSVT'23.
</li>
<li><p>2023-07: Our paper on <a href="https://dl.acm.org/doi/10.1145/3581783.3611980">"Rethinking Pseudo-Label-Based Unsupervised Person Re-ID with Hierarchical Prototype-based Graph"</a> accepted by ACMMM'23.
</li>

</ul>
<a name="research"></a>
<h2>Research</h2>
<p>Research topics are listed below. Full list of my publication can be found <a href="https://fansarah.github.io/publications">here</a>.</p> 
<p>
<h3>Hyperspectral Image Classification</h3>
</p>
<ul>
<li><p>Jiamu Sheng, Jingyi Zhou, Jiong Wang, Peng Ye, <b>Jiayuan Fan</b> <a href="https://ieeexplore.ieee.org/document/10798573">DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for Hyperspectral Image Classification</a>, <b>TGRS-24</b>. Code is available <a href="https://github.com/sjmFDU/DualMamba">here</a>.</p>
</li>    
<li><p>Jingyi Zhou, Jiamu Sheng, Peng Ye, <b>Jiayuan Fan</b>, Tong He, Bin Wang, Tao Chen <a href="https://ieeexplore.ieee.org/document/10542168">Exploring Multi-Timestep Multi-Stage Diffusion Features for Hyperspectral Image Classification</a>, <b>TGRS-24</b>. Code has been released <a href="https://github.com/zjyaccount/MTMSD/tree/main">here</a>.</p>
</li>
<li><p>Lin Zhan, Peng Ye, <b>Jiayuan Fan</b>, Tao Chen <a href="https://ieeexplore.ieee.org/document/10510343">U²ConvFormer: Marrying and Evolving Nested U-Net and Scale-Aware Transformer for Hyperspectral Image Classification</a>, <b>TGRS-24</b>.</p>
</li>
<li><p>Ke Wu, <b>Jiayuan Fan</b>, Peng Ye, Mingzhen Zhu <a href="https://ieeexplore.ieee.org/document/10075631">Hyperspectral Image Classification Using Spectral–Spatial Token Enhanced Transformer with Hash-Based Positional Embedding</a>, <b>TGRS-23</b>.</p>
</li>
<li><p>Mingzhen Zhu, <b>Jiayuan Fan</b>, Qihang Yang, Tao Chen <a href="https://ieeexplore.ieee.org/document/9627700">SC-EADNet: A Self-Supervised Contrastive Efficient Asymmetric Dilated Network for Hyperspectral Image Classification</a>, <b>TGRS-22</b>. Code can be found <a href="https://github.com/mZhenz/SC-EADNet">here</a>.</p>
</li>
<li><p><b>Jiayuan Fan</b>, Tao Chen, Shijian Lu <a href="https://ieeexplore.ieee.org/abstract/document/8022877/">Superpixel guided deep-sparse-representation learning for hyperspectral image classification</a>, <b>TCSVT-18</b>.</p>
</li>

</ul>
<p>
<h3>3D Spatial Intelligence</h3>
</p>
<ul>
<li><p>Wanzhang Li, Fukun Yin, Wen Liu, Yiying Yang, Xin Chen, Biao Jiang, Gang Yu, <b>Jiayuan Fan</b> <a href="https://ieeexplore.ieee.org/abstract/document/10747249/">Unbounded-GS: Extending 3D Gaussian Splatting with Hybrid Representation for Unbounded Large-Scale Scene Reconstruction</a>, <b>RAL-24</b>.</p>
</li>
<li><p>Yiying Yang, Fukun Yin, Wen Liu, <b>Jiayuan Fan</b>, Xin Chen, Gang Yu, Tao Chen <a href="https://dl.acm.org/doi/10.1609/aaai.v38i7.28481">Pm-inr: Prior-rich multi-modal implicit large-scale scene neural representation</a>, <b>AAAI-24</b>.</p>
</li>
<li><p>Yuhan Ding, Fukun Yin, <b>Jiayuan Fan</b>, Hui Li, Xin Chen, Wen Liu, Chongshan Lu, Gang Yu, Tao Chen <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/0073cc73e1873b35345209b50a3dab66-Paper-Conference.pdf">Point diffusion implicit function for large-scale scene neural representation</a>, <b>NeurIPS-23</b>.</p>
</li>
<li><p>Chongshan Lu, Fukun Yin, Xin Chen, Tao Chen, Gang YU, <b>Jiayuan Fan</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.pdf">A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction</a>, <b>ICCV-23</b>.</p>
</li>
</ul>
<p>
<h3>Embodied Vision Language Model</h3>
</p>
<ul>
<li><p>Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang Yu, <b>Jiayuan Fan</b> <a href="https://link.springer.com/chapter/10.1007/978-3-031-73347-5_4">Motionchain: Conversational motion controllers via multimodal prompts</a>, <b>ECCV-24</b>.</p>
</li>
<li><p>Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, <b>Jiayuan Fan</b>, Tao Chen <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Chen_LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni-3D_Understanding_Reasoning_and_CVPR_2024_paper.html">Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning</a>, <b>CVPR-23</b>.</p>
</li>
<li><p>Yongbin Liao, Hongyuan Zhu, Yanggang Zhang, Chuangguan Ye, Tao Chen, <b>Jiayuan Fan</b> <a href="https://ieeexplore.ieee.org/document/9628055">Point Cloud Instance Segmentation With Semi-Supervised Bounding-Box Mining</a>, <b>TPAMI-22</b>.</p>
</li>
</ul>
<h3>2D Visual Modeling</h3>
</p>
<ul>
<li><p>Jingdong Zhang, <b>Jiayuan Fan</b>, Peng Ye, Bo Zhang, Hancheng Ye, Baopu Li, Yancheng Cai, Tao Chen <a href="https://ieeexplore.ieee.org/abstract/document/10870147/">BridgeNet: Comprehensive and Effective Feature Interactions via Bridge Feature for Multi-Task Dense Predictions</a>, <b>TPAMI-25</b>.</p>
</li>
<li><p>Mingsheng Li, Lin Zhang, Mingzhen Zhu, Zilong Huang, Gang Yu, <b>Jiayuan Fan</b>, Tao Chen <a href="https://ieeexplore.ieee.org/abstract/document/10551493/">Lightweight model pre-training via language guided knowledge distillation</a>, <b>TMM-24</b>.</p>
</li>
<li><p>Lin Zhang, Bo Zhang, Botian Shi, <b>Jiayuan Fan</b>, Tao Chen <a href="https://ieeexplore.ieee.org/abstract/document/10516600/">Few-shot cross-domain object detection with instance-level prototype-based meta-learning</a>, <b>TCSVT-24</b>.</p>
</li>
<li><p>Zhen Mei, Peng Ye, Baopu Li, Tao Chen, <b>Jiayuan Fan</b>, Wanli Ouyang <a href="https://ieeexplore.ieee.org/abstract/document/10255628/">DeNKD: Decoupled Non-Target Knowledge Distillation for Complementing Transformer-Based Unsupervised Domain Adaptation</a>, <b>TCSVT-23</b>.</p>
</li>

</ul>

<!-- <a name="team"></a>
<h2>Team</h2>
<p>
<h3>Ph.D. Students</h3>
</p>
<ul>
<li><p><b>Fukun Yin</b></p>
</li>   
<li><p><b>Lin Zhang</b></p>
</li>   
<li><p><b>Jiakang Yuan</b></p>
</li>  
<li><p><b>Jianjian Cao</b></p> 
</li>
<li><p><b>Jiong Wang</b></p>
</li>
</ul> -->
<!-- <p>
<h3>Master Students</h3>
</p>
<ul>  
<li><p><b>Jiamu Sheng</b></p>
</li>  
<li><p><b>Zhende Song</b></p>
</li>  
<li><p><b>Mingyu Wu</b></p>
</li>
<li><p><b>Ziyuan Zhong</b></p>
</li>  
</li>
</ul> -->
<a name="teaching"></a>
<h2>Teaching</h2>
<ul>
<li><p><b>Computational Neuroscience </b>(Undergraduate)</p>
</li>
<li><p><b>Foundations of Machine Learning </b>(Postgraduate)</p>
</li>
<li><p><b>Image Processing </b>(Postgraduate)</p>
</li>  
</ul>
</tr>
</table>
</body>
</html>
